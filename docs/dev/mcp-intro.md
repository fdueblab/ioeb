# 模型上下文协议（MCP）技术深度解读

::: tip 协议概述
**模型上下文协议（Model Context Protocol，MCP）**是由 Anthropic 等推动的开放标准，旨在为大型语言模型（LLM）应用提供统一接口，将其与外部数据源、工具和服务连接起来。
:::

MCP 的提出旨在突破以往 LLM **信息孤岛**的局限，让模型不再仅依赖训练语料，而是能够在需要时动态获取实时的上下文信息、调用外部功能，从而生成更相关的回答或执行更复杂的任务。

::: info 核心理念
可以将 MCP 类比为 AI 应用的 **"USB-C 接口"**：不同的数据源和工具通过这一标准化接口接入，大幅降低每接入一个新服务就定制开发的成本，使 AI **助手具备即插即用的扩展能力**。
:::

下面将从协议结构、会话管理、上下文注入机制、与 LLM 的交互原理、指令/插件扩展、模型路由与动态内容拼接、安全与版本控制，以及生态实现等方面，对 MCP 的底层技术设计进行全面分析。

## 协议结构与通信方式

### 架构设计

**架构角色：** MCP 采用**客户端-服务器**架构，其中 *MCP 客户端* 嵌入在 LLM 应用一侧，*MCP 服务器* 运行在靠近数据源/工具的一侧，中间通过标准协议通信。

在典型场景下，LLM 所在的应用（例如 Claude Desktop、IDE 插件或聊天机器人等）作为 **主机（Host）** 发起连接，内部创建 MCP **客户端**实例，与一个或多个远程的 MCP **服务器**建立一对一连接。

::: warning 重要特性
每个服务器对接特定的数据源或功能模块，向客户端提供其**能力声明**（如支持资源、工具、提示模板等）。这种解耦设计使得LLM应用可以同时对接多个后台服务，获取丰富上下文。
:::

### 消息格式

**JSON-RPC 2.0 规范：** MCP 在通信层采用 **JSON-RPC 2.0** 规范作为消息的封装格式。所有请求、响应和通知消息均以 JSON 结构发送，包含 `jsonrpc` 版本号、方法名称、参数和请求ID等字段。

JSON-RPC 天然支持**请求-响应**和**单向通知**两种模式，MCP 传输层会将协议语义映射到 JSON-RPC 消息上进行交互。

**请求消息格式示例：**
```json
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "资源/读取",
  "params": { ... }
}
```

响应则包含对应的 `id` 以及 `result` 或 `error` 字段。这一选择使 MCP 具有**语言无关性和轻量级**的优点，便于各端实现。

### 传输通信机制

MCP 当前定义了两种**标准传输机制**：

#### 1. 标准输入/输出（stdio）传输

客户端与服务器通过本地进程的标准输入输出流通信。这种模式适合本地集成场景，例如将服务器作为子进程启动，与 IDE 插件或桌面应用在同一台机器上通信。

::: details 使用场景
Stdio 方式实现**简洁的进程内通讯**，开销低，常用于命令行工具或本地代理。
:::

#### 2. 可流式 HTTP 传输

基于 HTTP(S) 请求/响应的通信，支持 **Server-Sent Events (SSE)** 用于服务端推送流数据。

具体而言，客户端对每个 JSON-RPC 请求发起一个 HTTP POST 到服务器的 MCP 接口，服务器可以选择一次性以 JSON 响应，或以 `text/event-stream` 格式建立 SSE 流分多次推送结果。

::: tip HTTP 传输优势
- 支持云端部署
- 多客户端并发连接
- **有状态会话**保持
- 可在 HTTP/2 等协议上运行，获得更高并发性能
:::

### 连接与会话生命周期

MCP 连接是**有状态的持续会话**，包括初始化、消息交互、终止三个阶段：

1. **初始化阶段**：客户端首先发送 `initialize` 请求，声明所用的 MCP 协议版本及自身支持的功能；服务器收到后回应其支持的协议版本和功能集，然后客户端再发送 `initialized` 通知确认，握手完成

2. **消息交换阶段**：客户端和服务器可相互发送请求并获取响应，或发送单向通知进行状态更新

3. **终止阶段**：一方调用关闭或出现错误时，释放会话资源

::: info 会话特性
每个 MCP 会话通常对应一次用户会话或任务上下文，因此可以把**连接视为上下文会话**，在该连接中持续提供相关资源和工具而不混杂其他无关内容。
:::

## 上下文管理与注入机制

### 上下文资源曝光

MCP 的核心宗旨是在**LLM 与外部环境间注入有用的上下文**。为此，协议提供了 **"资源"（Resources）** 这一抽象来表示由服务器提供的各类数据内容。

资源可以是：
- 文件文本
- 数据库记录
- API 响应
- 日志片段
- 图片、音频等任意类型的信息

每个资源由 **URI** 唯一标识，URI 可以采用自定义方案表示不同来源（例如文件系统路径、数据库模式、屏幕截图等）。

### 资源管理机制

服务器通过实现 `resources/list` 请求，向客户端提供可用资源的列表，包括每个资源的 URI、名称、描述、类型（MIME）和大小等元数据。

当需要获取资源内容时，客户端发送 `resources/read` 请求，指定资源 URI，服务器即可返回其内容数据：

- **文本资源**：以 UTF-8 文本直接返回
- **二进制资源**：以 base64 编码传输

::: details 动态资源支持
为支持**动态资源**（如按需查询的内容），服务器还可提供 *URI 模板* 列表（`uriTemplate`），客户端填入参数即可构造有效资源 URI 请求。
:::

### 资源使用策略

MCP 设计中区分了**资源由谁来选择使用**。一般而言，资源是**由用户或应用控制**的：

- Claude Desktop 要求用户在界面上手动勾选哪些资源可供模型使用
- 某些客户端基于启发式自动帮用户选取相关资源
- 未来某些实现允许 AI 模型自主决定引用哪些资源

::: warning 设计原则
对于**模型需要自主获取**且无需用户逐一确认的上下文，MCP 提供了另一更主动的机制——**工具（Tools）**。
:::

### 上下文窗口交互原理

由于大型语言模型具有固定的最大提示长度，MCP 在设计上通过**分层控制**来缓解上下文窗口限制：

1. **客户端策展**：扮演"策展人"角色，从服务器提供的大量资源中选取**精炼子集**供模型阅读
2. **内容分页**：服务器可以将**长内容分页或概要化**
3. **根范围限制**：客户端在初始化时可以限定服务器的关注范围

::: tip 优势
MCP 通过资源列表+读取、范围限定、用户选择和模板化注入等机制，实现了与 LLM 上下文窗口的**高效协同**：既让模型能获取**尽可能丰富的外部信息**，又通过客户端层面的控制，**防止上下文超载**或引入不相关内容。
:::

## 指令与插件扩展机制

MCP 提供了灵活的**扩展机制**，允许为 LLM 增添可执行的功能和预定义的对话流程，相当于**插件系统**。主要体现在 **工具（Tools）** 和 **提示模板（Prompts）** 两种能力。

### 工具（Tools）

工具是在 MCP 服务器端暴露的一组可调用**函数/操作**，模型可以通过客户端调用这些工具来完成计算、查询或执行操作。

#### 工具特性

与资源不同，工具是**动态的、具有副作用**的操作，可以修改外部状态或获取实时信息，因此MCP将其设计为**模型控制**的扩展。

#### 工具使用流程

1. **工具发现**：服务器通过实现 `tools/list` 请求来让客户端获取其提供的所有工具清单
2. **工具注册**：每个工具定义包含唯一名称、描述以及 **输入参数模式**（JSON Schema）
3. **工具调用**：当模型决定调用某个工具时，客户端执行 `tools/call` 请求发送给服务器
4. **结果返回**：服务器执行真实操作后将结果返回

**工具定义示例：**
```json
{
  "name": "calculate_sum",
  "description": "将两数相加",
  "inputSchema": {
    "type": "object",
    "properties": {
      "a": {"type": "number"},
      "b": {"type": "number"}
    },
    "required": ["a", "b"]
  }
}
```

::: danger 安全考虑
工具往往意味着**任意代码执行**权限，因此规范要求 Host 应用在执行任何工具前都应征得用户明确授权，并确保用户了解工具的作用。
:::

### 提示模板（Prompts）

提示是 MCP 定义的另一类扩展机制，用于封装可重用的**对话模板、指令集合或多步骤工作流**。

#### 提示模板机制

1. **模板发现**：服务器通过 `prompts/list` 提供一系列预定义的提示
2. **模板调用**：用户选择或触发某个提示时，客户端发送 `prompts/get` 请求
3. **内容返回**：服务器返回该提示对应的**完整消息序列**

#### 动态上下文嵌入

提示模板支持**动态嵌入上下文**：服务器返回的对话消息中可以带有特殊内容类型表示引用某个资源内容。

**示例流程：**
1. 第一条消息：用户要求"分析日志和代码"
2. 第二条消息：内容类型为 `"resource"`，嵌入最近1小时的系统日志
3. 第三条消息：嵌入指定代码文件内容

::: info 使用特点
与资源、工具不同，提示模板通常是**用户控制**使用的扩展，即需要用户明确选择何时使用哪个模板。
:::

### 指令注入与系统消息

MCP 本身并未定义专有的"系统角色指令"接口，但在与模型交互时，Host 侧可以利用模型原生提供的系统消息通道。

例如：
- **OpenAI Agents SDK**：将工具列表以**函数定义**形式注入模型上下文
- **Anthropic Claude**：通过特定提示格式传达工具可用信息
- **Sampling 功能**：服务器可以建议一个 systemPrompt（系统提示）

## 多模型路由与动态内容拼接

MCP 被业界视为 AI 领域的重要开放接口之一，不仅因为它连接数据与模型，还因为其设计有助于**多模型协作与动态响应组装**。

### 模型无关的上下文接口

**跨模型兼容性：** MCP 将工具和资源的接口标准化，使开发者可以**在不同 LLM 后端之间切换**而无须更改集成逻辑。

同样的 MCP 服务器可以同时服务于不同的客户端：
- OpenAI GPT 系列模型的客户端
- Anthropic Claude 的客户端

这意味着构建在 MCP 之上的应用可以灵活采用不同模型来完成不同任务——**动态模型路由**：
- 需要高精度推理时用更强大的模型
- 简单任务用快速小模型

### Sampling 机制与代理闭环

MCP 定义的 **"采样"**（Sampling）功能允许服务器**反过来请求客户端的 LLM 生成内容**。

#### Sampling 工作流程

1. **服务器请求**：发送 `sampling/createMessage` 请求，附带它希望 LLM 执行的对话消息序列和参数
2. **用户审核**：客户端收到后由用户审核
3. **模型生成**：调用自身的 LLM模型生成回复
4. **结果返回**：把结果返回服务器

#### 上下文包含控制

在采样请求中，服务器可以指定是否包含 MCP 上下文，以及包含范围：

- `"includeContext": "thisServer"`：附加当前服务器已有的上下文信息
- `"allServers"`：包括来自所有已连接服务器的上下文

::: tip 动态内容拼接示例
一个"MCP服务器A"负责产品数据库查询，另一个"MCP服务器B"负责用户评论分析。服务器B可以通过 sampling 请求要求模型依据来自服务器A的产品数据和自身的评论数据一起生成报告。
:::

### 多模态支持

MCP 的架构天然可扩展到文字以外的内容形态：

- **二进制数据传输**：资源内容类型中允许图像的 base64 编码
- **多模态消息**：工具执行结果和采样消息都支持 type 为 `"image"`
- **未来扩展**：路线图中提到将支持视频等更多模态

### Agent 图与多代理体系

MCP 规范的未来演进方向之一是**代理图**（Agent Graph）。这暗示支持复杂的多 Agent 拓扑，每个 Agent 之间通过命名空间和图通信模式协作。

::: info 多智能体协作
在这种模型下，可能存在一个**调度代理**，它本身通过 MCP 访问工具/数据，又控制多个**子代理模型**处理不同子任务。MCP 提供的标准上下文共享和工具调用接口，将使这些代理间更易交换信息和调用彼此能力。
:::

## 安全性、鉴权与版本控制

### 安全设计原则

MCP 协议赋予 AI 模型读取私有数据、执行外部操作的强大能力，也带来了相应的安全与信任挑战。MCP 规范从原则上要求实现者遵循**"用户掌控"**和**"最小信任"**的准则。

#### 核心安全原则

1. **用户知情同意**：用户必须对 AI 应用能访问哪些数据、能调用哪些操作拥有**最终决定权**

2. **数据隐私与保护**：主机在未获用户许可时，**不得**将用户数据发送给 MCP 服务器

3. **工具安全**：工具等同远程代码执行，必须**谨慎对待**

4. **模型调用控制**：当服务器通过 Sampling 请求模型完成子任务时，Host 也应让用户确认

::: danger 重要提醒
上述原则无法靠协议本身强制，需要**实现层面**遵循。因此 MCP 在文档中特别强调应用开发者应构建可靠的**授权流程**、记录安全文档、设置访问控制和速率限制等。
:::

### 传输安全

对于远程 HTTP 模式的服务器，应采取以下安全措施：

- **TLS 加密**：保证通道安全
- **API Key、OAuth**：对服务器进行**鉴权**，防止未授权方接入
- **输入验证**：服务器需做好输入验证、路径校验，避免模型发来的请求导致本地系统的安全漏洞

### 版本兼容与标准化

MCP 作为新兴开放协议，正处于快速迭代中。目前官方通过**版本号和日期**对规范进行版本控制。

#### 版本演进

- **2024年11月25日**：发布首个开源规范
- **2025年3月26日**：发布更新版本，对一些功能进行了改进

#### 版本协商

在初始化 handshake 阶段，客户端和服务器会交换各自的协议版本，如果不匹配则需采取兼容方案或拒绝连接。

#### 标准化进程

- **社区驱动**：MCP 项目采用类似 IETF 的标准跟踪流程
- **开放治理**：Anthropic 明确表示希望通过**社区驱动**来演进 MCP，并计划探索通过行业标准组织进行正式标准化

### 开放生态与主导权

MCP 是开源项目，所有规范、SDK和示例代码均在 GitHub 开放。

#### 生态参与者

- **Anthropic**：目前是主要贡献方和维护者
- **早期采用者**：Block、Apollo、Replit、Codeium、Sourcegraph 等
- **大厂支持**：OpenAI、Google DeepMind 等重量级玩家

::: info 行业认可
连 Google DeepMind CEO 哈萨比斯也在社交媒体表示对 MCP 作为开放标准的认可与支持。随着多家一流 AI 企业的参与，MCP 有望朝真正**跨厂商的行业标准**迈进。
:::

## MCP 工具与SDK生态概览

MCP 发布以来迅速建立了多语言、多平台的**开发工具链**。官方和社区提供了丰富的 SDK、集成库和示例服务器。

### 主要实现对比

| **实现/工具** | **提供方/来源** | **语言/环境** | **主要特性** |
|-------------|---------------|-------------|-------------|
| **官方 MCP SDK** | Anthropic 开源 | Python、TypeScript、Java、Kotlin、C#、Swift、Ruby 等 | 提供完整的 MCP 客户端和服务器框架，实现协议规范全功能 |
| **OpenAI Agents SDK (MCP)** | OpenAI 开源 | Python | OpenAI 智能体框架的 MCP 支持模块，可将 MCP 服务器作为工具接入 GPT 模型 |
| **Spring AI MCP 集成** | Spring 社区 | Java/Spring Boot | 在 Spring 应用中快速启用 MCP 功能的工具包 |
| **LangChain MCP Adapter** | LangChain 社区 | Python | 为 LangChain 框架提供 MCP 支持的适配器 |
| **官方示例连接器** | Anthropic & 开源社区 | 多语言（主要TypeScript/Python） | 覆盖 Google Drive、Slack、GitHub、Git、PostgreSQL、Puppeteer 等常见数据源 |

### 生态现状分析

::: tip 生态优势
- **多语言支持**：官方 SDK 覆盖主流开发语言，降低了实现门槛
- **平台兼容**：主流 AI 平台和框架纷纷支持或兼容 MCP
- **开箱即用**：丰富的示例连接器提供即插即用的能力
:::

正如 Anthropic 产品官所言，"MCP 已成为蓬勃发展的开放标准，已有成千上万的集成，并在不断增长"。

## 总结

Model Context Protocol（MCP）通过标准化 LLM 与外部世界交互的协议，解决了长期困扰 AI 应用的**上下文封闭**和**集成碎片化**问题。

### 技术贡献

本文深入分析了 MCP 的底层技术机制：

1. **协议层面**：从 JSON-RPC 消息格式、stdio/HTTP 通信模式
2. **功能模块**：资源/工具/提示等功能模块如何注入上下文
3. **高级特性**：多模型协作、动态内容组装和安全治理

### 设计理念

MCP 的设计借鉴了语言服务器协议等成熟模式，以**开放、灵活、可扩展**为出发点，使 AI 模型能够在用户掌控下接入任意所需的数据和能力。

### 行业意义

作为一项仍在演进的开放标准，MCP 已显示出巨大的潜力：

::: info 行业价值
- **打破厂商壁垒**：为 AI 行业提供了一个共同语言，促进协作
- **简化开发**：让开发者告别繁琐的定制集成，转而专注于高级应用逻辑
- **标准化意义**：有望成为 AI 应用领域的"USB-C接口"
:::

### 未来展望

虽然 MCP 目前也引发了热烈讨论——支持者认为其标准化意义重大，批评者则提醒不要忽视简单方案的价值，但随着 OpenAI、DeepMind 等的加入以及社区持续完善，MCP 的规范必将更加成熟。

在可预见的将来，MCP 或将成为连接模型与世界各类信息系统的桥梁，推动更**自主智能**的应用诞生。对此，我们拭目以待。

---

## 参考文献

[1] [模型上下文协议 - 维基百科](https://zh.wikipedia.org/zh-hans/模型上下文协议)

[2] [Introducing the Model Context Protocol - Anthropic](https://www.anthropic.com/news/model-context-protocol)

[3] [MCP 官方文档 - Model Context Protocol](https://modelcontextprotocol.io)

[4] [OpenAI Agents SDK - MCP 支持](https://openai.github.io/openai-agents-python/mcp/)

[5] [OpenAI 拥抱 Anthropic MCP 协议，引发社区热议 - 无限社区](https://www.xinfinite.net/t/topic/11214)

[6] [MCP协议由Anthropic推出 - 搜狐](https://www.sohu.com/a/882028752_121956424)

[7] [Spring AI MCP 集成 - 博客园](https://www.cnblogs.com/jingzh/p/18814165)

[8] [MCP Adapters for LangChain - LangChain](https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph)

[9] [MCP: Flash in the Pan or Fixture? - LangChain Blog](https://blog.langchain.dev/mcp-fad-or-fixture/)

[10] [模型上下文协议连接器 - Adobe Experience League](https://experienceleague.adobe.com/zh-hans/docs/workfront-fusion/using/references/apps-and-their-modules/tools-and-transformers/model-context-protocol-mcp-connector)

[11] [MCP 是一个开放协议 - Anthropic 文档](https://docs.anthropic.com/zh-CN/docs/agents-and-tools/mcp)

[12] [MCP协议的主要特点和优势 - ShowAPI](https://www.showapi.com/news/article/67e4e44a4ddd791c0e0014ca)

[13] [Google A2A 与Anthropic MCP 该如何选择？ - CSDN](https://blog.csdn.net/Baihai_IDP/article/details/147625656)

[14] [谷歌宣布支持Anthropic的MCP协议 - 东方财富网](https://wap.eastmoney.com/a/202504113373731284.html) 